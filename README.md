# ğŸ  Seoul Apartment Price Prediction

> ì„œìš¸ì‹œ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ íšŒê·€ ëª¨ë¸ ê°œë°œ í”„ë¡œì íŠ¸

## ğŸ“‹ Overview

| Item | Detail |
|------|--------|
| **Period** | 2026.01.06 - 2026.01.11 |
| **Task** | Regression (House Price Prediction) |
| **Goal** | ì¼ë°˜í™”ëœ ëª¨ë¸ì„ ê°œë°œí•˜ì—¬ ì•„íŒŒíŠ¸ ì‹œì¥ì˜ ë™í–¥ì„ ë¯¸ë¦¬ ì˜ˆì¸¡ |
| **Result** | RMSE 68.7% ê°œì„  (47,133 â†’ 14,751) |
| **Stack** | Python, LightGBM, XGBoost, Scikit-Learn |

---

## ğŸ“Š Dataset

**Source**: êµ­í† êµí†µë¶€ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ë°ì´í„° + ì„œìš¸ì‹œ ì§€í•˜ì² ì—­/ë²„ìŠ¤ì •ë¥˜ì¥ ì •ë³´ ë°ì´í„°

| Dataset | Period | Size |
|---------|--------|------|
| Train | 2007.01 ~ 2023.06 | 1,118,822 rows, 52 features |
| Test | 2023.07 ~ 2023.09 | 9,272 rows |

---

## ğŸ” Approach

### 1. EDA

- **Log ë³€í™˜**: ë¶€ë™ì‚° ê±°ë˜ íŠ¹ì„±ìƒ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¼¬ë¦¬ê°€ ê¸´ (Right Skewed) ë¶„í¬ í™•ì¸ â†’ yë³€ìˆ˜ì— ë¡œê·¸ ë³€í™˜ ì ìš©
- **ê²°ì¸¡ì¹˜ ë³´ì™„**: ì¹´ì¹´ì˜¤ë§µ APIë¡œ ì•„íŒŒíŠ¸ì˜ ìœ„ë„/ê²½ë„ ì¢Œí‘œ ê²°ì¸¡ì¹˜ ë³´ì™„ (ì•½ 88ë§Œê±´)

### 2. Feature Engineering

**ì¢Œí‘œ ê¸°ë°˜ ê±°ë¦¬ íŒŒìƒë³€ìˆ˜**

Feature Importance ë¶„ì„ì—ì„œ ì¢Œí‘œ ì •ë³´ì˜ ì¤‘ìš”ì„±ì„ í™•ì¸í•˜ê³ , í•˜ë²„ì‚¬ì¸(Haversine) ê³µì‹ì„ í™œìš©í•˜ì—¬ ì£¼ìš” ê±°ì ê¹Œì§€ì˜ ê±°ë¦¬ ë³€ìˆ˜ë¥¼ ìƒì„±:

$$d = 2R \cdot \arctan2\left(\sqrt{a},\ \sqrt{1-a}\right)$$

$$a = \sin^2\left(\frac{\Delta\phi}{2}\right) + \cos(\phi_1)\cos(\phi_2)\sin^2\left(\frac{\Delta\lambda}{2}\right)$$

- ê°•ë‚¨ì—­/ì••êµ¬ì •ì—­/ì‹œì²­ì—­ê¹Œì§€ì˜ ê±°ë¦¬ (km)
- 500m ë°˜ê²½ ë‚´ ì—­/ì •ë¥˜ì¥ ìˆ˜, ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥ê¹Œì§€ì˜ ê±°ë¦¬

**íƒ€ê²Ÿ ì¸ì½”ë”©**

- êµ¬ë³„ í‰ê· ê°€, ë™ë³„ í‰ê· ê°€, ë™ë³„ í‰ë‹¹ê°€, ë™ë³„ ìµœê·¼ 5ë…„ í‰ë‹¹ê°€

**ê¸°íƒ€**
- ê°•ë‚¨3êµ¬, gu_group(êµ¬ë³„ ë§¤ë§¤ê°€ mean, stdë¡œ kmeansìˆ˜í–‰í•˜ì—¬ 5ê°œ ê·¸ë£¹í™”)
- ê±´ë¬¼ì—°ì‹, ì‹ ì¶•ì—¬ë¶€, within5yrs, within3yrs, over_30yrs, over_40yrs, over_50yrs
- ê³ ì¸µ*ë©´ì , í‰í˜•ëŒ€ ë²”ì£¼í™”(ì†Œ/ì¤‘ì†Œ/ì¤‘ëŒ€/ëŒ€í˜•)
- ê¸ˆë¦¬
- ë¶„ê¸°

### 3. Model Selection

| Model | Initial Valid RMSE |
|-------|-------------|
| Random Forest | 5850.79 |
| XGBoost | 4403.02 |
| **LightGBM** | **4114.19** |
| ì•™ìƒë¸” (XGB+LGBM) | 4225.73 |

- XGBoost ëŒ€ë¹„ ëŒ€ìš©ëŸ‰ ë°ì´í„°(111ë§Œê±´)ì— ì í•©í•œ LightGBMì˜ í•™ìŠµ ì†ë„/ì„±ëŠ¥ ìš°ìˆ˜
- Model Selection í›„ LightGBM ë‹¨ì¼ëª¨ë¸ë¡œ ìµœì¢… ì„ ì •

### 4. Validation Strategy

**Time Series Split Cross Validation** ì ìš©

- Random Split ì ìš© ì‹œ ë¯¸ë˜ë°ì´í„°ë¡œ ê³¼ê±°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì˜¤ë¥˜ ë°œê²¬
- ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê°ì•ˆí•˜ì—¬ ì‹œê°„ ìˆœì„œ ë³´ì¥

```
Train set : 2007.01 ~ 2022.12
Valid set : 2023.01 ~ 2023.06
```

### 5. Hyperparameter Tuning

LightGBM ë‹¨ì¼ëª¨ë¸ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜ ë°˜ë³µ:

```python
final_params = {
    'n_estimators': 20000,
    'learning_rate': 0.005,
    'num_leaves': 127,
    'min_child_samples': 20,
    'feature_fraction': 0.8,
    'objective': 'regression',
    'metric': 'rmse'
}
```

### 6. Feature Selection

Feature Importance ë¶„ì„ í†µí•´ ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ 22ê°œ ì„ íƒ:

> íŒŒìƒë³€ìˆ˜ê°€ ë§ë‹¤ê³  í•´ì„œ ëª¨ë¸ ì„±ëŠ¥ì´ ê°œì„ ë˜ëŠ” ê²ƒì€ ì•„ë‹˜ â†’ Feature Selection ì¤‘ìš”


![Feature Importance](images/feature_importance.png)

---

## ğŸ“ˆ Result
RMSE: 47,133 â†’ 14,751 (68.7% ê°œì„ )

---

## ğŸ’¡ Key Insights

1. ë¹ ë¥¸ ì‹¤í—˜ í™•ì¸ ê°€ëŠ¥í•˜ë©° ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•œ **LightGBMì˜ íŠ¹ì„±** í™•ì¸
2. ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ **Cross Validation ì „ëµ ìˆ˜ì •**ì´ ì¤‘ìš”í•¨
3. íŒŒìƒë³€ìˆ˜ê°€ ë§ë‹¤ê³  í•´ì„œ ëª¨ë¸ ì„±ëŠ¥ì´ ê°œì„ ë˜ëŠ” ê²ƒì€ ì•„ë‹˜
4. ìƒê´€ë¶„ì„, Feature Importance ë“± í†µí•´ì„œ **ìœ ì˜ë¯¸í•œ Feature Selection** ì¤‘ìš”

---

## ğŸ›  Tech Stack

![Python](https://img.shields.io/badge/Python-3.10-3776AB?logo=python&logoColor=white)
![LightGBM](https://img.shields.io/badge/LightGBM-4.1-9ACD32)
![XGBoost](https://img.shields.io/badge/XGBoost-2.0-FF6600)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.2-F7931E?logo=scikit-learn&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-1.5-150458?logo=pandas&logoColor=white)
